{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229 - Problem Set 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Newtons method for computing least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ J(\\theta) = \\dfrac{1}{2} \\sum_{i=1}^{m} \\left( \\theta^T x^{(i)} - y^{(i)} \\right)^2 = \\dfrac{1}{2} \\big|\\big| \\, C \\, \\big|\\big|^2, \\; C \\in \\mathbb{R}^{m}, \\; C = \\begin{bmatrix}\n",
    "    \\theta^T x^{(1)} - y^{(1)} \\\\\n",
    "    \\theta^T x^{(2)} - y^{(2)} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta^T x^{(m)} - y^{(m)} \n",
    "    \\end{bmatrix} $,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, $ X = \\begin{bmatrix} x^{(1)} \\\\ x^{(2)} \\\\ \\vdots \\\\ x^{(m)} \\end{bmatrix} $, $ X \\in \\mathbb{R}^{m \\times n} $, $ Y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix} $, $ Y \\in \\mathbb{R}^{m} $, hence, $ C = X \\theta - Y $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, $ J(\\theta) = \\dfrac{1}{2} \\big|\\big| \\, X \\theta - Y \\, \\big|\\big|^2 = \\dfrac{1}{2} \\left( X \\theta - Y \\right)^T \\left( X \\theta - Y \\right) = \\dfrac{1}{2} \\left( \\theta^T X^T - Y^T \\right) \\left( X \\theta - Y \\right) = \\dfrac{1}{2} \\left( \\theta^T X^T X \\theta - \\theta^T X^T Y - Y^T X \\theta + Y^T Y \\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ = \\dfrac{1}{2} \\left( \\theta^T X^T X \\theta - \\left( Y^T X \\theta \\right)^T - Y^T X \\theta + Y^T Y \\right) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $ Y^T X \\theta \\in \\mathbb{R} $, $ J(\\theta) = \\dfrac{1}{2} \\left( \\theta^T X^T X \\theta - 2 Y^T X \\theta + Y^T Y \\right) $. Hessian with respect to $ \\theta $ can be then given as $\\mathrm{Hess}(J) = X^T X $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is given as $ J(\\theta) = \\dfrac{1}{2} \\left( \\theta^T X^T X \\theta - 2 Y^T X \\theta + Y^T Y \\right) $. The global minimum can be obtained by setting the gradient to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\nabla_\\theta J = X^T X \\theta - X^T Y = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ X^T X \\theta = X^T Y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ (X^T X)^{-1} X^T X \\theta = (X^T X)^{-1} X^T Y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\theta = (X^T X)^{-1} X^T Y $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration in Newton's method can be represented as $ \\theta^{(n+1)} = \\theta^{(n)} - (\\mathrm{Hess}(J(\\theta^{(n)})))^{-1} \\nabla_\\theta J(\\theta^{(n)}) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\theta^{(n+1)} = \\theta^{(n)} - ( X^T X )^{-1} ( X^T X \\theta^{(n)} - X^T Y ) = \\theta^{(n)} - ( X^T X )^{-1} (X^T X) \\theta^{(n)} + ( X^T X )^{-1}(X^T Y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\theta^{(n+1)} = ( X^T X )^{-1}X^T Y  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Locally-weighted logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient and the Hessian of the cost function will be derived first, as suggested in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is given by $ \\mathcal{l}(\\theta) = -\\dfrac{\\lambda}{2}\\theta^T \\theta + \\sum_{i=1}^{m} w^{(i)} \\left[ y^{(i)} \\log \\left( h_{\\theta} \\left( x^{(i)} \\right) \\right) + \\left( 1-y^{(i)} \\right) \\log \\left( 1- h_{\\theta} \\left( x^{(i)} \\right) \\right) \\right] $ with $ h_{\\theta}( x ) = \\dfrac{1}{1 + e^{-\\theta^T x}}  $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ z^{(i)} = w^{(i)} \\left[ y^{(i)} \\log \\left( h_{\\theta} \\left( x^{(i)} \\right) \\right) + \\left( 1-y^{(i)} \\right) \\log \\left( 1- h_{\\theta} \\left( x^{(i)} \\right) \\right) \\right] $. Then the gradient $ \\nabla_{\\theta} \\left( l(\\theta) \\right) = \\lambda \\theta + \\sum_{i=1}^{m} \\nabla_{\\theta} \\left( z^{(i)} \\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\nabla_{\\theta} \\left( z^{(i)} \\right) = w^{(i)} \\left[ y^{(i)} \\nabla_{\\theta} \\left( \\log \\left( h_{\\theta}\\left( x^{(i)} \\right) \\right) \\right) + \\left( 1-y^{(i)} \\right) \\nabla_{\\theta} \\left( \\log \\left( 1- h_{\\theta}\\left( x^{(i)} \\right) \\right) \\right) \\right] = w^{(i)} \\left[ \\nabla_{\\theta}\\left( h_{\\theta} \\left( x^{(i)} \\right) \\right) \\dfrac{y^{(i)}}{h_{\\theta}}  \\right] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
